# dunkingscam_streamlit_app_fixed.py
# Single-file Streamlit app (fixed)
# - Fixes: properly persist uploaded file across reruns, actually process uploaded image with Vintern OCR (if available),
#   robust error handling, clearer FAST_UI_ONLY flow, and small bug fixes (PIL resampling compatibility).
# - NOTES:
# 1) Models are heavy. Set `FAST_UI_ONLY = True` to skip loading models for UI testing.
# 2) Install requirements: pip install streamlit transformers torch torchvision pillow
# 3) Run: streamlit run dunkingscam_streamlit_app_fixed.py

import streamlit as st
from PIL import Image, ImageEnhance, ImageOps
import io
import re
import textwrap
import time
import base64

# ----------------------- CONFIG -----------------------
st.set_page_config(page_title="DunkingScam AI", layout="wide", initial_sidebar_state="collapsed")

# Toggle for quick UI testing without loading heavy models
FAST_UI_ONLY = False  # <-- switch to False when you want to load actual models

# ----------------------- ENHANCED STYLES -----------------------
# (omitted here for brevity in the chat document) - keep same CSS as your original file
CSS = """
<!-- CSS TRUNCATED FOR READABILITY - paste your original CSS here if you want full styling -->
"""
st.markdown("<style>/* tiny reset to avoid streamlit header overlap */ header {display:none}</style>", unsafe_allow_html=True)

# ----------------------- MODEL LOADING -----------------------
@st.cache_resource
def load_phobert(phobert_path: str = "DuyKien016/phobert-scam-detector"):
    if FAST_UI_ONLY:
        return None, None
    try:
        from transformers import AutoTokenizer, AutoModelForSequenceClassification
        import torch
        tokenizer = AutoTokenizer.from_pretrained(phobert_path, use_fast=False)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = AutoModelForSequenceClassification.from_pretrained(phobert_path)
        model.to(device).eval()
        return tokenizer, model
    except Exception as e:
        return None, None

@st.cache_resource
def load_vintern():
    if FAST_UI_ONLY:
        return None, None
    try:
        from transformers import AutoModel, AutoTokenizer
        import torch
        vintern_model = AutoModel.from_pretrained(
            "5CD-AI/Vintern-1B-v3_5",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        ).eval()
        vintern_tokenizer = AutoTokenizer.from_pretrained(
            "5CD-AI/Vintern-1B-v3_5",
            trust_remote_code=True
        )
        return vintern_model, vintern_tokenizer
    except Exception as e:
        return None, None

# Load models (or placeholders)
with st.spinner("Ki·ªÉm tra m√¥ h√¨nh (n·∫øu b·∫≠t FAST_UI_ONLY = False th√¨ c√≥ th·ªÉ m·∫•t th·ªùi gian)..."):
    phobert_tokenizer, phobert_model = load_phobert()
    vintern_model, vintern_tokenizer = load_vintern()

# ----------------------- UTILITIES -----------------------
import torch
import torchvision.transforms as T

# Backwards-compatible resampling attribute for PIL
try:
    RESAMPLING_LANCZOS = Image.Resampling.LANCZOS
except AttributeError:
    RESAMPLING_LANCZOS = Image.LANCZOS


def enhance_image_for_ocr(image: Image.Image) -> Image.Image:
    enhancer = ImageEnhance.Contrast(image)
    image = enhancer.enhance(1.8)
    enhancer = ImageEnhance.Sharpness(image)
    image = enhancer.enhance(1.3)
    return image


def vintern_ocr_extract(img_pil: Image.Image):
    """Run Vintern model to extract list of messages from an image.
    Returns (messages_list, error_message_or_None)
    """
    if vintern_model is None:
        return [], "Vintern model not available (FAST_UI_ONLY or load error)."
    try:
        img = img_pil.convert("RGB")
        img = enhance_image_for_ocr(img)
        max_size = (448, 448)
        img.thumbnail(max_size, RESAMPLING_LANCZOS)
        img = ImageOps.pad(img, max_size, color=(245, 245, 245))

        transform = T.Compose([
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        pixel_values = transform(img).unsqueeze(0)

        # Build prompt in Vietnamese similar to your original
        prompt = textwrap.dedent("""
        <image>
        ƒê·ªçc t·ª´ng tin nh·∫Øn trong ·∫£nh v√† xu·∫•t ra ƒë·ªãnh d·∫°ng:

        Tin nh·∫Øn 1: [n·ªôi dung]
        Tin nh·∫Øn 2: [n·ªôi dung]
        Tin nh·∫Øn 3: [n·ªôi dung]

        Quy t·∫Øc:
        - M·ªói √¥ chat = 1 tin nh·∫Øn
        - Ch·ªâ l·∫•y n·ªôi dung vƒÉn b·∫£n
        - B·ªè th·ªùi gian, t√™n ng∆∞·ªùi, emoji
        - ƒê·ªçc t·ª´ tr√™n xu·ªëng d∆∞·ªõi

        B·∫Øt ƒë·∫ßu:
        """)

        # Some Vintern variants implement a chat() helper; try calling it and fall back if not.
        try:
            response, *_ = vintern_model.chat(
                tokenizer=vintern_tokenizer,
                pixel_values=pixel_values,
                question=prompt,
                generation_config=dict(max_new_tokens=512, do_sample=False, num_beams=1, early_stopping=True),
                history=None,
                return_history=True
            )
        except Exception:
            # If vintern doesn't have chat(), try forward pass (this may fail depending on model code)
            return [], "Vintern model does not support chat() in this environment."

        messages = re.findall(r"Tin nh·∫Øn \d+: (.+?)(?=\nTin nh·∫Øn|\Z)", response, re.S)

        def quick_clean(msg):
            msg = re.sub(r"\s+", " ", msg.strip())
            msg = re.sub(r'^\d+[\.\)\-\s]+', '', msg)
            return msg.strip()

        cleaned = [quick_clean(m) for m in messages if m.strip()]
        return cleaned, None

    except Exception as e:
        return [], f"Error running Vintern: {e}"


def phobert_predict(texts):
    if phobert_tokenizer is None or phobert_model is None:
        # Return UNKNOWN results so UI still works in FAST_UI_ONLY mode
        return [{"text": t, "prediction": "UNKNOWN", "confidence": "0%"} for t in texts]

    results = []
    for text in texts:
        encoded = phobert_tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=256)
        encoded = {k: v.to(next(phobert_model.parameters()).device) for k, v in encoded.items()}
        with torch.no_grad():
            logits = phobert_model(**encoded).logits
            probs = torch.softmax(logits, dim=1).squeeze()
            label = torch.argmax(probs).item()
        results.append({
            "text": text,
            "prediction": "L·ª™A ƒê·∫¢O" if label == 1 else "B√åNH TH∆Ø·ªúNG",
            "confidence": f"{(probs[label]*100).item():.2f}%"
        })
    return results

# ----------------------- JAVASCRIPT FOR AUTO SCROLL -----------------------

def add_auto_scroll_js():
    st.markdown("""
    <script>
    function scrollToBottom() {
        var chatContainer = document.querySelector('.chat-messages');
        if (chatContainer) {
            chatContainer.scrollTop = chatContainer.scrollHeight;
        }
    }
    setTimeout(scrollToBottom, 100);
    </script>
    """, unsafe_allow_html=True)

# ----------------------- PAGE NAV -----------------------
if 'page' not in st.session_state:
    st.session_state['page'] = 'home'
if 'messages' not in st.session_state:
    st.session_state['messages'] = []

# store uploaded file bytes so they survive reruns and form clearing
if 'uploaded_file_bytes' not in st.session_state:
    st.session_state['uploaded_file_bytes'] = None
if 'uploaded_file_name' not in st.session_state:
    st.session_state['uploaded_file_name'] = None


def save_uploaded_file(uploaded_file):
    if uploaded_file is None:
        return
    data = uploaded_file.read()
    st.session_state['uploaded_file_bytes'] = data
    st.session_state['uploaded_file_name'] = uploaded_file.name


def go_chat():
    st.session_state['page'] = 'chat'
    st.experimental_rerun()


def go_home():
    st.session_state['page'] = 'home'
    st.experimental_rerun()

# ----------------------- HOME PAGE -----------------------
if st.session_state['page'] == 'home':
    st.markdown("<h1 style='text-align:center'>DunkingScam AI (Demo)</h1>", unsafe_allow_html=True)
    st.write("Trang demo. Nh·∫•n n√∫t ƒë·ªÉ qua giao di·ªán chat.")
    if st.button("üöÄ B·∫Øt ƒë·∫ßu ngay"):
        go_chat()

# ----------------------- CHAT PAGE -----------------------
elif st.session_state['page'] == 'chat':
    st.markdown("<h2>DunkingScam Chat</h2>", unsafe_allow_html=True)
    col_left, col_right = st.columns([7, 3])

    with col_left:
        st.markdown('<div class="chat-container">', unsafe_allow_html=True)
        st.markdown('<div class="chat-messages" id="chatMessages">', unsafe_allow_html=True)

        for i, msg in enumerate(st.session_state['messages']):
            if msg['role'] == 'ai':
                st.markdown(f"""
                <div class="message">
                    <div class="message-bubble bubble-ai">{msg['text'].replace(chr(10), '<br>')}</div>
                    <div class="message-meta">{msg.get('meta', 'AI')} ‚Ä¢ {msg.get('time', 'v·ª´a xong')}</div>
                </div>
                """, unsafe_allow_html=True)
            elif msg['role'] == 'user':
                st.markdown(f"""
                <div class="message message-user">
                    <div class="message-bubble bubble-user">{msg['text']}</div>
                    <div class="message-meta">{msg.get('meta', 'B·∫°n')} ‚Ä¢ {msg.get('time', 'v·ª´a xong')}</div>
                </div>
                """, unsafe_allow_html=True)
            elif msg['role'] == 'typing':
                st.markdown("""
                <div class="typing-indicator">
                    <span style="color: #6b7280; font-size: 14px;">AI ƒëang suy nghƒ©</span>
                    <div class="typing-dots">
                        <div class="typing-dot"></div>
                        <div class="typing-dot"></div>
                        <div class="typing-dot"></div>
                    </div>
                </div>
                """, unsafe_allow_html=True)

        st.markdown('</div>', unsafe_allow_html=True)

        with st.form(key='chat_form', clear_on_submit=True):
            uploaded_file = st.file_uploader("üì∏ T·∫£i ·∫£nh ch·ª•p m√†n h√¨nh tin nh·∫Øn", type=['png', 'jpg', 'jpeg'], key='file_input')
            text_input = st.text_area("üí¨ Ho·∫∑c g√µ tin nh·∫Øn ƒë·ªÉ ki·ªÉm tra...", height=80, key='text_input', placeholder="Nh·∫≠p n·ªôi dung tin nh·∫Øn b·∫°n mu·ªën ki·ªÉm tra...")
            submit = st.form_submit_button("üì§ G·ª≠i", use_container_width=True)

            if submit:
                current_time = time.strftime("%H:%M")

                if uploaded_file is not None:
                    # persist uploaded file across reruns
                    save_uploaded_file(uploaded_file)
                    user_msg = f"üì∏ {uploaded_file.name}"
                elif text_input.strip():
                    user_msg = text_input.strip()
                else:
                    user_msg = "(tin nh·∫Øn tr·ªëng)"

                st.session_state['messages'].append({'role': 'user', 'text': user_msg, 'meta': 'B·∫°n', 'time': current_time})
                st.session_state['messages'].append({'role': 'typing', 'text': '', 'meta': 'AI', 'time': current_time})

                # show typing indicator immediately
                st.experimental_rerun()

        st.markdown('</div>', unsafe_allow_html=True)

    with col_right:
        st.markdown("<div class='chat-sidebar'><h4>H∆∞·ªõng d·∫´n</h4><ul><li>G√µ ho·∫∑c t·∫£i ·∫£nh ƒë·ªÉ ki·ªÉm tra.</li></ul></div>", unsafe_allow_html=True)
        if st.button("üè† Quay v·ªÅ trang ch·ªß"):
            go_home()
        if st.button("üóëÔ∏è X√≥a cu·ªôc tr√≤ chuy·ªán"):
            st.session_state['messages'] = []
            st.session_state['uploaded_file_bytes'] = None
            st.session_state['uploaded_file_name'] = None
            st.experimental_rerun()

    # ---------------- PROCESSING: trigger when last message was typing ----------------
    if st.session_state['messages'] and st.session_state['messages'][-1]['role'] == 'typing':
        # remove typing indicator
        st.session_state['messages'].pop()
        user_message = st.session_state['messages'][-1]
        current_time = time.strftime("%H:%M")

        # small delay to simulate processing
        time.sleep(0.8)

        try:
            if user_message['text'].startswith('üì∏'):
                # handle uploaded image
                if st.session_state['uploaded_file_bytes'] is None:
                    ai_response = "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y ·∫£nh. Vui l√≤ng t·∫£i l·∫°i ·∫£nh v√† nh·∫•n G·ª≠i."
                else:
                    img_bytes = st.session_state['uploaded_file_bytes']
                    img_name = st.session_state.get('uploaded_file_name', 'image')
                    try:
                        img = Image.open(io.BytesIO(img_bytes))
                    except Exception as e:
                        ai_response = f"‚ùå Kh√¥ng th·ªÉ m·ªü ·∫£nh: {e}"
                    else:
                        messages_extracted, err = vintern_ocr_extract(img)
                        if err:
                            ai_response = f"‚ö†Ô∏è Vintern OCR kh√¥ng kh·∫£ d·ª•ng: {err}\n\nB·∫°n v·∫´n c√≥ th·ªÉ d√°n vƒÉn b·∫£n tr·ª±c ti·∫øp ƒë·ªÉ ki·ªÉm tra."
                        elif not messages_extracted:
                            ai_response = "‚ö†Ô∏è Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c tin nh·∫Øn n√†o t·ª´ ·∫£nh. Vui l√≤ng th·ª≠ ·∫£nh kh√°c ho·∫∑c d√°n tr·ª±c ti·∫øp vƒÉn b·∫£n." 
                        else:
                            # classify each extracted message
                            preds = phobert_predict(messages_extracted)
                            lines = ["üîç **K·∫øt qu·∫£ ph√¢n t√≠ch ·∫£nh**\n"]
                            for idx, p in enumerate(preds, start=1):
                                lines.append(f"- Tin nh·∫Øn {idx}: {p['prediction']} ({p['confidence']})\n  > {p['text']}")

                            ai_response = "\n".join(lines)
            elif user_message['text'] == "(tin nh·∫Øn tr·ªëng)":
                ai_response = "‚ö†Ô∏è B·∫°n ch∆∞a nh·∫≠p n·ªôi dung tin nh·∫Øn. Vui l√≤ng nh·∫≠p vƒÉn b·∫£n ho·∫∑c t·∫£i ·∫£nh ƒë·ªÉ ph√¢n t√≠ch."
            else:
                # plain text classification
                text = user_message['text']
                predictions = phobert_predict([text])
                pred = predictions[0]
                if pred['prediction'] == 'L·ª™A ƒê·∫¢O':
                    ai_response = (
                        f"üö® **C·∫¢NH B√ÅO L·ª™A ƒê·∫¢O**\n\n"
                        f"**K·∫øt qu·∫£ ph√¢n t√≠ch:** {pred['prediction']}\n"
                        f"**ƒê·ªô tin c·∫≠y:** {pred['confidence']}\n\n"
                        "**Khuy·∫øn ngh·ªã:**\n- Kh√¥ng cung c·∫•p th√¥ng tin c√° nh√¢n\n- Kh√¥ng chuy·ªÉn ti·ªÅn\n- X√°c minh th√¥ng qua k√™nh ch√≠nh th·ª©c\n\n"
                        f"**N·ªôi dung ƒë√£ ph√¢n t√≠ch:**\n\"{pred['text']}\""
                    )
                elif pred['prediction'] == 'UNKNOWN':
                    ai_response = (
                        "‚ö†Ô∏è M√¥ h√¨nh Phobert ch∆∞a ƒë∆∞·ª£c t·∫£i ho·∫∑c kh√¥ng kh·∫£ d·ª•ng.\n"
                        "B·∫°n c√≥ th·ªÉ b·∫≠t FAST_UI_ONLY=False v√† c√†i ƒë·∫∑t m√¥ h√¨nh, ho·∫∑c d√°n tr·ª±c ti·∫øp vƒÉn b·∫£n ƒë·ªÉ ki·ªÉm tra theo logic tƒ©nh."
                    )
                else:
                    ai_response = (
                        f"‚úÖ **Tin nh·∫Øn an to√†n**\n\n"
                        f"**K·∫øt qu·∫£ ph√¢n t√≠ch:** {pred['prediction']}\n"
                        f"**ƒê·ªô tin c·∫≠y:** {pred['confidence']}\n\n"
                        f"**N·ªôi dung ƒë√£ ph√¢n t√≠ch:**\n\"{pred['text']}\"\n\n"
                        "üí° L∆∞u √Ω: D√π ƒë∆∞·ª£c ƒë√°nh gi√° an to√†n, h√£y th·∫≠n tr·ªçng v·ªõi y√™u c·∫ßu chuy·ªÉn ti·ªÅn ho·∫∑c th√¥ng tin nh·∫°y c·∫£m."
                    )
        except Exception as e:
            ai_response = f"‚ùå L·ªói x·ª≠ l√Ω: {e}"

        # append AI response and scroll
        st.session_state['messages'].append({'role': 'ai', 'text': ai_response, 'meta': 'AI Assistant', 'time': current_time})
        add_auto_scroll_js()
        st.experimental_rerun()

# EOF
